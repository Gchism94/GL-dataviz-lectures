{
  "hash": "9933cc0cee522259f689363f1b2f3fb3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: PCA + Clustering\nsubtitle: INFO Data Visualization and Analysis - Week 5\ninstitute: UArizona School of Information\nauthor: Dr. Greg Chism\ntitle-slide-attributes:\n  data-background-image: minedata-bg.png\n  data-background-size: '600px, cover'\n  data-slide-number: none\nformat:\n  revealjs:\n    theme: slides.scss\n    transition: fade\n    slide-number: true\n    chalkboard: true\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\nauto-stretch: false\nfooter: \"[\\U0001F517 GL-dataviz-lectures](https://gchism94.github.io/GL-dataviz-lectures)\"\n---\n\n## Setup\n\n::: {#setup .cell message='false' execution_count=1}\n``` {.python .cell-code}\n# Data Manipulation and Analysis\nimport pandas as pd\nimport numpy as np\n\n# Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Machine Learning\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Statistical Analysis\nimport statsmodels.api as sm\nimport scipy.stats as stats\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale=1.25)\n\n# Set Seaborn theme\nsns.set_theme(style=\"whitegrid\", palette=\"colorblind\")\n```\n:::\n\n\n## Data Preprocessing\n\n> **Data preprocessing** can refer to manipulation, filtration or augmentation of data before it is analyzed, and is often an important step in the data analysis process.\n\n## Datasets {.smaller}\n\n\n\n**Human Freedom Index**\n\nThe Human Freedom Index is a report that attempts to summarize the idea of \"freedom\" through variables for many countries around the globe.\n\n::: {#0e6a8ff7 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](week5_files/figure-revealjs/cell-4-output-1.png){width=692 height=307}\n:::\n:::\n\n\n## Our data: Human Freedom Index {.smaller}\n\n::: {#bf6ae82e .cell execution_count=4}\n``` {.python .cell-code}\nhfi = pd.read_csv(\"data/hfi.csv\")\nhfi.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>ISO_code</th>\n      <th>countries</th>\n      <th>region</th>\n      <th>pf_rol_procedural</th>\n      <th>pf_rol_civil</th>\n      <th>pf_rol_criminal</th>\n      <th>pf_rol</th>\n      <th>pf_ss_homicide</th>\n      <th>pf_ss_disappearances_disap</th>\n      <th>...</th>\n      <th>ef_regulation_business_bribes</th>\n      <th>ef_regulation_business_licensing</th>\n      <th>ef_regulation_business_compliance</th>\n      <th>ef_regulation_business</th>\n      <th>ef_regulation</th>\n      <th>ef_score</th>\n      <th>ef_rank</th>\n      <th>hf_score</th>\n      <th>hf_rank</th>\n      <th>hf_quartile</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2016</td>\n      <td>ALB</td>\n      <td>Albania</td>\n      <td>Eastern Europe</td>\n      <td>6.661503</td>\n      <td>4.547244</td>\n      <td>4.666508</td>\n      <td>5.291752</td>\n      <td>8.920429</td>\n      <td>10.0</td>\n      <td>...</td>\n      <td>4.050196</td>\n      <td>7.324582</td>\n      <td>7.074366</td>\n      <td>6.705863</td>\n      <td>6.906901</td>\n      <td>7.54</td>\n      <td>34.0</td>\n      <td>7.568140</td>\n      <td>48.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2016</td>\n      <td>DZA</td>\n      <td>Algeria</td>\n      <td>Middle East &amp; North Africa</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.819566</td>\n      <td>9.456254</td>\n      <td>10.0</td>\n      <td>...</td>\n      <td>3.765515</td>\n      <td>8.523503</td>\n      <td>7.029528</td>\n      <td>5.676956</td>\n      <td>5.268992</td>\n      <td>4.99</td>\n      <td>159.0</td>\n      <td>5.135886</td>\n      <td>155.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2016</td>\n      <td>AGO</td>\n      <td>Angola</td>\n      <td>Sub-Saharan Africa</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.451814</td>\n      <td>8.060260</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>1.945540</td>\n      <td>8.096776</td>\n      <td>6.782923</td>\n      <td>4.930271</td>\n      <td>5.518500</td>\n      <td>5.17</td>\n      <td>155.0</td>\n      <td>5.640662</td>\n      <td>142.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2016</td>\n      <td>ARG</td>\n      <td>Argentina</td>\n      <td>Latin America &amp; the Caribbean</td>\n      <td>7.098483</td>\n      <td>5.791960</td>\n      <td>4.343930</td>\n      <td>5.744791</td>\n      <td>7.622974</td>\n      <td>10.0</td>\n      <td>...</td>\n      <td>3.260044</td>\n      <td>5.253411</td>\n      <td>6.508295</td>\n      <td>5.535831</td>\n      <td>5.369019</td>\n      <td>4.84</td>\n      <td>160.0</td>\n      <td>6.469848</td>\n      <td>107.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2016</td>\n      <td>ARM</td>\n      <td>Armenia</td>\n      <td>Caucasus &amp; Central Asia</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.003205</td>\n      <td>8.808750</td>\n      <td>10.0</td>\n      <td>...</td>\n      <td>4.575152</td>\n      <td>9.319612</td>\n      <td>6.491481</td>\n      <td>6.797530</td>\n      <td>7.378069</td>\n      <td>7.57</td>\n      <td>29.0</td>\n      <td>7.241402</td>\n      <td>57.0</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 123 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Understand the data {.smaller}\n\n::: panel-tabset\n## `.info()`\n\n::: {#e5705a94 .cell execution_count=5}\n``` {.python .cell-code}\nhfi.info(verbose = True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1458 entries, 0 to 1457\nData columns (total 123 columns):\n #    Column                              Dtype  \n---   ------                              -----  \n 0    year                                int64  \n 1    ISO_code                            object \n 2    countries                           object \n 3    region                              object \n 4    pf_rol_procedural                   float64\n 5    pf_rol_civil                        float64\n 6    pf_rol_criminal                     float64\n 7    pf_rol                              float64\n 8    pf_ss_homicide                      float64\n 9    pf_ss_disappearances_disap          float64\n 10   pf_ss_disappearances_violent        float64\n 11   pf_ss_disappearances_organized      float64\n 12   pf_ss_disappearances_fatalities     float64\n 13   pf_ss_disappearances_injuries       float64\n 14   pf_ss_disappearances                float64\n 15   pf_ss_women_fgm                     float64\n 16   pf_ss_women_missing                 float64\n 17   pf_ss_women_inheritance_widows      float64\n 18   pf_ss_women_inheritance_daughters   float64\n 19   pf_ss_women_inheritance             float64\n 20   pf_ss_women                         float64\n 21   pf_ss                               float64\n 22   pf_movement_domestic                float64\n 23   pf_movement_foreign                 float64\n 24   pf_movement_women                   float64\n 25   pf_movement                         float64\n 26   pf_religion_estop_establish         float64\n 27   pf_religion_estop_operate           float64\n 28   pf_religion_estop                   float64\n 29   pf_religion_harassment              float64\n 30   pf_religion_restrictions            float64\n 31   pf_religion                         float64\n 32   pf_association_association          float64\n 33   pf_association_assembly             float64\n 34   pf_association_political_establish  float64\n 35   pf_association_political_operate    float64\n 36   pf_association_political            float64\n 37   pf_association_prof_establish       float64\n 38   pf_association_prof_operate         float64\n 39   pf_association_prof                 float64\n 40   pf_association_sport_establish      float64\n 41   pf_association_sport_operate        float64\n 42   pf_association_sport                float64\n 43   pf_association                      float64\n 44   pf_expression_killed                float64\n 45   pf_expression_jailed                float64\n 46   pf_expression_influence             float64\n 47   pf_expression_control               float64\n 48   pf_expression_cable                 float64\n 49   pf_expression_newspapers            float64\n 50   pf_expression_internet              float64\n 51   pf_expression                       float64\n 52   pf_identity_legal                   float64\n 53   pf_identity_parental_marriage       float64\n 54   pf_identity_parental_divorce        float64\n 55   pf_identity_parental                float64\n 56   pf_identity_sex_male                float64\n 57   pf_identity_sex_female              float64\n 58   pf_identity_sex                     float64\n 59   pf_identity_divorce                 float64\n 60   pf_identity                         float64\n 61   pf_score                            float64\n 62   pf_rank                             float64\n 63   ef_government_consumption           float64\n 64   ef_government_transfers             float64\n 65   ef_government_enterprises           float64\n 66   ef_government_tax_income            float64\n 67   ef_government_tax_payroll           float64\n 68   ef_government_tax                   float64\n 69   ef_government                       float64\n 70   ef_legal_judicial                   float64\n 71   ef_legal_courts                     float64\n 72   ef_legal_protection                 float64\n 73   ef_legal_military                   float64\n 74   ef_legal_integrity                  float64\n 75   ef_legal_enforcement                float64\n 76   ef_legal_restrictions               float64\n 77   ef_legal_police                     float64\n 78   ef_legal_crime                      float64\n 79   ef_legal_gender                     float64\n 80   ef_legal                            float64\n 81   ef_money_growth                     float64\n 82   ef_money_sd                         float64\n 83   ef_money_inflation                  float64\n 84   ef_money_currency                   float64\n 85   ef_money                            float64\n 86   ef_trade_tariffs_revenue            float64\n 87   ef_trade_tariffs_mean               float64\n 88   ef_trade_tariffs_sd                 float64\n 89   ef_trade_tariffs                    float64\n 90   ef_trade_regulatory_nontariff       float64\n 91   ef_trade_regulatory_compliance      float64\n 92   ef_trade_regulatory                 float64\n 93   ef_trade_black                      float64\n 94   ef_trade_movement_foreign           float64\n 95   ef_trade_movement_capital           float64\n 96   ef_trade_movement_visit             float64\n 97   ef_trade_movement                   float64\n 98   ef_trade                            float64\n 99   ef_regulation_credit_ownership      float64\n 100  ef_regulation_credit_private        float64\n 101  ef_regulation_credit_interest       float64\n 102  ef_regulation_credit                float64\n 103  ef_regulation_labor_minwage         float64\n 104  ef_regulation_labor_firing          float64\n 105  ef_regulation_labor_bargain         float64\n 106  ef_regulation_labor_hours           float64\n 107  ef_regulation_labor_dismissal       float64\n 108  ef_regulation_labor_conscription    float64\n 109  ef_regulation_labor                 float64\n 110  ef_regulation_business_adm          float64\n 111  ef_regulation_business_bureaucracy  float64\n 112  ef_regulation_business_start        float64\n 113  ef_regulation_business_bribes       float64\n 114  ef_regulation_business_licensing    float64\n 115  ef_regulation_business_compliance   float64\n 116  ef_regulation_business              float64\n 117  ef_regulation                       float64\n 118  ef_score                            float64\n 119  ef_rank                             float64\n 120  hf_score                            float64\n 121  hf_rank                             float64\n 122  hf_quartile                         float64\ndtypes: float64(119), int64(1), object(3)\nmemory usage: 1.4+ MB\n```\n:::\n:::\n\n\n## `.describe()`\n\n::: {#3d1628f3 .cell execution_count=6}\n``` {.python .cell-code}\nhfi.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>pf_rol_procedural</th>\n      <th>pf_rol_civil</th>\n      <th>pf_rol_criminal</th>\n      <th>pf_rol</th>\n      <th>pf_ss_homicide</th>\n      <th>pf_ss_disappearances_disap</th>\n      <th>pf_ss_disappearances_violent</th>\n      <th>pf_ss_disappearances_organized</th>\n      <th>pf_ss_disappearances_fatalities</th>\n      <th>...</th>\n      <th>ef_regulation_business_bribes</th>\n      <th>ef_regulation_business_licensing</th>\n      <th>ef_regulation_business_compliance</th>\n      <th>ef_regulation_business</th>\n      <th>ef_regulation</th>\n      <th>ef_score</th>\n      <th>ef_rank</th>\n      <th>hf_score</th>\n      <th>hf_rank</th>\n      <th>hf_quartile</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1458.000000</td>\n      <td>880.000000</td>\n      <td>880.000000</td>\n      <td>880.000000</td>\n      <td>1378.000000</td>\n      <td>1378.000000</td>\n      <td>1369.000000</td>\n      <td>1378.000000</td>\n      <td>1279.000000</td>\n      <td>1378.000000</td>\n      <td>...</td>\n      <td>1283.000000</td>\n      <td>1357.000000</td>\n      <td>1368.000000</td>\n      <td>1374.000000</td>\n      <td>1378.000000</td>\n      <td>1378.000000</td>\n      <td>1378.000000</td>\n      <td>1378.000000</td>\n      <td>1378.000000</td>\n      <td>1378.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2012.000000</td>\n      <td>5.589355</td>\n      <td>5.474770</td>\n      <td>5.044070</td>\n      <td>5.309641</td>\n      <td>7.412980</td>\n      <td>8.341855</td>\n      <td>9.519458</td>\n      <td>6.772869</td>\n      <td>9.584972</td>\n      <td>...</td>\n      <td>4.886192</td>\n      <td>7.698494</td>\n      <td>6.981858</td>\n      <td>6.317668</td>\n      <td>7.019782</td>\n      <td>6.785610</td>\n      <td>76.973149</td>\n      <td>6.993444</td>\n      <td>77.007983</td>\n      <td>2.490566</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.582875</td>\n      <td>2.080957</td>\n      <td>1.428494</td>\n      <td>1.724886</td>\n      <td>1.529310</td>\n      <td>2.832947</td>\n      <td>3.225902</td>\n      <td>1.744673</td>\n      <td>2.768983</td>\n      <td>1.559826</td>\n      <td>...</td>\n      <td>1.889168</td>\n      <td>1.728507</td>\n      <td>1.979200</td>\n      <td>1.230988</td>\n      <td>1.027625</td>\n      <td>0.883601</td>\n      <td>44.540142</td>\n      <td>1.025811</td>\n      <td>44.506549</td>\n      <td>1.119698</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>2008.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.009841</td>\n      <td>2.483540</td>\n      <td>2.880000</td>\n      <td>1.000000</td>\n      <td>3.765827</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2010.000000</td>\n      <td>4.133333</td>\n      <td>4.549550</td>\n      <td>3.789724</td>\n      <td>4.131746</td>\n      <td>6.386978</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>5.000000</td>\n      <td>9.942607</td>\n      <td>...</td>\n      <td>3.433786</td>\n      <td>6.874687</td>\n      <td>6.368178</td>\n      <td>5.591851</td>\n      <td>6.429498</td>\n      <td>6.250000</td>\n      <td>38.000000</td>\n      <td>6.336685</td>\n      <td>39.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2012.000000</td>\n      <td>5.300000</td>\n      <td>5.300000</td>\n      <td>4.575189</td>\n      <td>4.910797</td>\n      <td>8.638278</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>7.500000</td>\n      <td>10.000000</td>\n      <td>...</td>\n      <td>4.418371</td>\n      <td>8.074161</td>\n      <td>7.466692</td>\n      <td>6.265234</td>\n      <td>7.082075</td>\n      <td>6.900000</td>\n      <td>77.000000</td>\n      <td>6.923840</td>\n      <td>76.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2014.000000</td>\n      <td>7.389499</td>\n      <td>6.410975</td>\n      <td>6.400000</td>\n      <td>6.513178</td>\n      <td>9.454402</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>...</td>\n      <td>6.227978</td>\n      <td>8.991882</td>\n      <td>8.209310</td>\n      <td>7.139718</td>\n      <td>7.720955</td>\n      <td>7.410000</td>\n      <td>115.000000</td>\n      <td>7.894660</td>\n      <td>115.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2016.000000</td>\n      <td>9.700000</td>\n      <td>8.773533</td>\n      <td>8.719848</td>\n      <td>8.723094</td>\n      <td>9.926568</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>...</td>\n      <td>9.623811</td>\n      <td>9.999638</td>\n      <td>9.865488</td>\n      <td>9.272600</td>\n      <td>9.439828</td>\n      <td>9.190000</td>\n      <td>162.000000</td>\n      <td>9.126313</td>\n      <td>162.000000</td>\n      <td>4.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 120 columns</p>\n</div>\n```\n:::\n:::\n\n\n:::\n\n## Identifying missing values {.smaller}\n\n::: {#472962b9 .cell execution_count=7}\n``` {.python .cell-code}\nhfi.isna().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nyear                   0\nISO_code               0\ncountries              0\nregion                 0\npf_rol_procedural    578\n                    ... \nef_score              80\nef_rank               80\nhf_score              80\nhf_rank               80\nhf_quartile           80\nLength: 123, dtype: int64\n```\n:::\n:::\n\n\n::: fragment\n> A lot of missing values ðŸ™ƒ\n:::\n\n# Data Cleaning\n\n## Handling missing data\n\n#### Options\n\n::: incremental\n-   Do nothing...\n-   Remove them\n-   **Imputate**\n:::\n\n::: fragment\nWe will be using `pf_score` from `hsi`: 80 missing values\n:::\n\n## Imputation\n\n> In [statistics](https://en.wikipedia.org/wiki/Statistics \"Statistics\"), **imputation** is the process of replacing [missing data](https://en.wikipedia.org/wiki/Missing_data \"Missing data\") with substituted values.\n\n::: fragment\n#### Considerations\n\n::: incremental\n-   Data distribution\n-   Impact on analysis\n-   Missing data mechanism\n-   Multiple imputation\n-   Can also be used on **outliers**\n:::\n:::\n\n## Mean imputation {.smaller}\n\n::: panel-tabset\n## Definition\n\n**How it Works**: Replace missing values with the arithmetic **mean** of the non-missing values in the same variable.\n\n::: fragment\n**Pros**:\n\n::: incremental\n-   Easy and fast.\n-   Works well with small numerical datasets\n:::\n\n**Cons**:\n\n::: incremental\n-   It only works on the column level.\n-   Will give poor results on encoded categorical features.\n-   Not very accurate.\n-   Doesn't account for the uncertainty in the imputations.\n:::\n:::\n\n## Visual\n\n::: {#e0a2e3d0 .cell fig.asp='0.625' execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](week5_files/figure-revealjs/cell-9-output-1.png){width=824 height=435}\n:::\n:::\n\n\n## Code\n\n::: {#aa1ae243 .cell execution_count=9}\n``` {.python .cell-code code-line-numbers=\"1-12|1|3,4|6,8,10,12\"}\nhfi_copy = hfi\n\nmean_imputer = SimpleImputer(strategy = 'mean')\nhfi_copy['mean_pf_score'] = mean_imputer.fit_transform(hfi_copy[['pf_score']])\n\nmean_plot = sns.kdeplot(data = hfi_copy, x = 'pf_score', linewidth = 2, label = \"Original\")\n\nmean_plot = sns.kdeplot(data = hfi_copy, x = 'mean_pf_score', linewidth = 2, label = \"Mean Imputated\")\n\nplt.legend()\n\nplt.show()\n```\n:::\n\n\n:::\n\n## Median imputation {.smaller}\n\n::: panel-tabset\n## Definition\n\n**How it Works**: Replace missing values with the **median** of the non-missing values in the same variable.\n\n::: fragment\n**Pros** (same as mean):\n\n::: incremental\n-   Easy and fast.\n-   Works well with small numerical datasets\n:::\n\n**Cons** (same as mean):\n\n::: incremental\n-   It only works on the column level.\n-   Will give poor results on encoded categorical features.\n-   Not very accurate.\n-   Doesn't account for the uncertainty in the imputations.\n:::\n:::\n\n## Visual\n\n::: {#d5c1d7fe .cell fig.asp='0.625' ref.label='mean_imp' execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![](week5_files/figure-revealjs/cell-11-output-1.png){width=824 height=435}\n:::\n:::\n\n\n## Code\n\n::: {#aff5db81 .cell execution_count=11}\n``` {.python .cell-code code-line-numbers=\"1-10|1,2\"}\nmedian_imputer = SimpleImputer(strategy = 'median')\nhfi_copy['median_pf_score'] = median_imputer.fit_transform(hfi_copy[['pf_score']])\n\nmedian_plot = sns.kdeplot(data = hfi_copy, x = 'pf_score', linewidth = 2, label = \"Original\")\n\nmedian_plot = sns.kdeplot(data = hfi_copy, x = 'median_pf_score', linewidth = 2, label = \"Median Imputated\")\n\nplt.legend()\n\nplt.show()\n```\n:::\n\n\n:::\n\n## Data type conversion {.smaller}\n\n::: {#eeb61189 .cell execution_count=12}\n``` {.python .cell-code}\nhfi['year'] = pd.to_datetime(hfi['year'], format='%Y')\n\nhfi.head(1)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>ISO_code</th>\n      <th>countries</th>\n      <th>region</th>\n      <th>pf_rol_procedural</th>\n      <th>pf_rol_civil</th>\n      <th>pf_rol_criminal</th>\n      <th>pf_rol</th>\n      <th>pf_ss_homicide</th>\n      <th>pf_ss_disappearances_disap</th>\n      <th>...</th>\n      <th>ef_regulation_business_compliance</th>\n      <th>ef_regulation_business</th>\n      <th>ef_regulation</th>\n      <th>ef_score</th>\n      <th>ef_rank</th>\n      <th>hf_score</th>\n      <th>hf_rank</th>\n      <th>hf_quartile</th>\n      <th>mean_pf_score</th>\n      <th>median_pf_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2016-01-01</td>\n      <td>ALB</td>\n      <td>Albania</td>\n      <td>Eastern Europe</td>\n      <td>6.661503</td>\n      <td>4.547244</td>\n      <td>4.666508</td>\n      <td>5.291752</td>\n      <td>8.920429</td>\n      <td>10.0</td>\n      <td>...</td>\n      <td>7.074366</td>\n      <td>6.705863</td>\n      <td>6.906901</td>\n      <td>7.54</td>\n      <td>34.0</td>\n      <td>7.56814</td>\n      <td>48.0</td>\n      <td>2.0</td>\n      <td>7.596281</td>\n      <td>7.596281</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows Ã— 125 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#404ccdb0 .cell execution_count=13}\n``` {.python .cell-code}\nhfi.dtypes\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nyear                 datetime64[ns]\nISO_code                     object\ncountries                    object\nregion                       object\npf_rol_procedural           float64\n                          ...      \nhf_score                    float64\nhf_rank                     float64\nhf_quartile                 float64\nmean_pf_score               float64\nmedian_pf_score             float64\nLength: 125, dtype: object\n```\n:::\n:::\n\n\n## Removing duplicates {.smaller}\n\n::: {#d8d45a86 .cell execution_count=14}\n``` {.python .cell-code}\nhfi.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1458 entries, 0 to 1457\nColumns: 125 entries, year to median_pf_score\ndtypes: datetime64[ns](1), float64(121), object(3)\nmemory usage: 1.4+ MB\n```\n:::\n:::\n\n\n::: {#3b960327 .cell execution_count=15}\n``` {.python .cell-code}\nhfi.drop_duplicates(inplace = True)\nhfi.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1458 entries, 0 to 1457\nColumns: 125 entries, year to median_pf_score\ndtypes: datetime64[ns](1), float64(121), object(3)\nmemory usage: 1.4+ MB\n```\n:::\n:::\n\n\n::: fragment\n> No duplicates! ðŸ˜Š\n:::\n\n## Dimensional reduction {.smaller}\n\n> **Dimension reduction**, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its [intrinsic dimension](https://en.wikipedia.org/wiki/Intrinsic_dimension \"Intrinsic dimension\").\n\n::: fragment\n**Principal component analysis (PCA)** - Unsupervised\n\n::: incremental\n-   Maximizes variance in the dataset.\n\n-   Finds orthogonal principal components.\n\n-   Useful for feature extraction and data visualization.\n:::\n:::\n\n## Dimensional reduction: applied {.smaller}\n\n::: panel-tabset\n## Prep\n\n::: {#8bca993d .cell execution_count=16}\n``` {.python .cell-code}\nnumeric_cols = hfi.select_dtypes(include = [np.number]).columns\n\n# Applying mean imputation only to numeric columns\nhfi[numeric_cols] = hfi[numeric_cols].fillna(hfi[numeric_cols].mean())\n\nfeatures = ['pf_rol_procedural', 'pf_rol_civil', 'pf_rol_criminal', 'pf_rol', 'hf_score', 'hf_rank', 'hf_quartile']\n\nx = hfi.loc[:, features].values\ny = hfi.loc[:, 'region'].values\nx = StandardScaler().fit_transform(x)\n```\n:::\n\n\n## PCA: variance\n\n::: {#6fcf8f1f .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\" code-line-numbers=\"1-5|1|2|3|4|5\"}\npca = PCA(n_components = 2)\nprincipalComponents = pca.fit_transform(x)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\npca_variance_explained = pca.explained_variance_ratio_\nprint(\"Variance explained:\", pca_variance_explained, \"\\n\", principalDf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVariance explained: [0.76138995 0.15849799] \n       principal component 1  principal component 2\n0              5.164625e-01          -9.665680e-01\n1             -2.366765e+00           1.957381e+00\n2             -2.147729e+00           1.664483e+00\n3             -2.784437e-01           8.066415e-01\n4              3.716205e-01          -4.294282e-01\n...                     ...                    ...\n1453          -4.181375e+00          -4.496988e-01\n1454          -5.213024e-01           6.010449e-01\n1455           1.374342e-16          -2.907121e-16\n1456          -1.545577e+00          -5.422255e-01\n1457          -3.669011e+00           4.294948e-01\n\n[1458 rows x 2 columns]\n```\n:::\n:::\n\n\n## PCA: scree plot\n\n::: {#e1b1d0e3 .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\n# Combining the scatterplot of principal components with the scree plot using the correct column names\nfig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 5))\n\n# Scatterplot of Principal Components\naxes[0].scatter(principalDf['principal component 1'], principalDf['principal component 2'])\nfor i in range(len(pca.components_)):\n    axes[0].arrow(0, 0, pca.components_[i, 0], pca.components_[i, 1], head_width = 0.1, head_length = 0.15, fc = 'r', ec = 'r', linewidth = 2)\n    axes[0].text(pca.components_[i, 0] * 1.2, pca.components_[i, 1] * 1.2, f'Eigenvector {i+1}', color = 'r', fontsize = 12)\naxes[0].set_xlabel('Principal Component 1')\naxes[0].set_ylabel('Principal Component 2')\naxes[0].set_title('Scatterplot of Principal Components with Eigenvectors')\naxes[0].grid()\n\n# Scree Plot for PCA\naxes[1].bar(range(1, len(pca_variance_explained) + 1), pca_variance_explained, alpha = 0.6, color = 'g', label = 'Individual Explained Variance')\naxes[1].set_ylabel('Explained variance ratio')\naxes[1].set_xlabel('Principal components')\naxes[1].set_title('Scree Plot for PCA')\naxes[1].legend(loc='best')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](week5_files/figure-revealjs/cell-19-output-1.png){width=1136 height=463}\n:::\n:::\n\n\n:::\n\n# So, that's it?\n\n## ...Not really {.smaller}\n\nFind the optimal number of components.\n\n::: {#8bc4abe4 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\n# Assuming hfi DataFrame is already defined and loaded\n\n# Select numerical columns\nnumerical_cols = hfi.select_dtypes(include=['int64', 'float64']).columns\n\n# Scale the data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(hfi[numerical_cols])\n\n# Apply PCA\npca = PCA().fit(scaled_data)\n\n# Get explained variance ratio and cumulative explained variance\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_explained_variance = explained_variance_ratio.cumsum()\n\n# Decide number of components to retain 75% variance\nthreshold = 0.75\nnum_components = next(i for i, cumulative_var in enumerate(cumulative_explained_variance) if cumulative_var >= threshold) + 1\n\n# Plot the explained variance\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\nplt.axhline(y=threshold, color='r', linestyle='-')\nplt.axvline(x=num_components, color='r', linestyle='-')\nplt.annotate(f'{num_components} components', xy=(num_components, threshold), xytext=(num_components+5, threshold-0.05),\n             arrowprops=dict(color='r', arrowstyle='->'),\n             fontsize=12, color='r')\nplt.title('Cumulative Explained Variance by Principal Components')\nplt.xlabel('Principal Component')\nplt.ylabel('Cumulative Explained Variance')\nplt.grid(True)\nplt.show()\n\nprint(f\"Number of components to retain 75% variance: {num_components}\")\n\n# Apply PCA with the chosen number of components\npca = PCA(n_components=num_components)\nreduced_data = pca.fit_transform(scaled_data)\n```\n\n::: {.cell-output .cell-output-display}\n![](week5_files/figure-revealjs/cell-20-output-1.png){width=816 height=529}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of components to retain 75% variance: 19\n```\n:::\n:::\n\n\n## Dimensional reduction: what now? {.smaller}\n\n::: incremental\n1.  **Feature Selection:** Choose the most informative components.\n\n2.  **Visualization:** Graph the reduced dimensions to identify patterns.\n\n3.  **Clustering:** Group similar data points using clustering algorithms.\n\n4.  **Classification:** Predict categories using classifiers on reduced features.\n\n5.  **Model Evaluation:** Assess model performance with metrics like accuracy.\n\n6.  **Cross-Validation:** Validate model stability with cross-validation.\n\n7.  **Hyperparameter Tuning:** Optimize model settings for better performance.\n\n8.  **Model Interpretation:** Understand feature influence in the models.\n\n9.  **Ensemble Methods:** Improve predictions by combining multiple models.\n\n10. **Deployment:** Deploy the model for real-world predictions.\n\n11. **Iterative Refinement:** Refine analysis based on initial results.\n\n12. **Reporting:** Summarize findings for stakeholders.\n:::\n\n# Clustering\n\n## Setup {.smaller}\n\n::: {#setup-1 .cell message='false' execution_count=20}\n``` {.python .cell-code}\n# Data Handling and Manipulation\nimport pandas as pd\nimport numpy as np\n\n# Data Preprocessing\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\n\n# Model Selection and Evaluation\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\nfrom sklearn.mixture import GaussianMixture\n\n# Clustering Models\nfrom sklearn.cluster import KMeans\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the default style for visualization\nsns.set_theme(style = \"white\", palette = \"colorblind\")\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)\n```\n:::\n\n\n## Unsupervised Learning\n\n## \n\n<br>\n\n![Credit: Recro](images/unsupervised.jpeg){fig-align=\"center\" width=\"1528\"}\n\n## Clustering\n\n![](images/clustering-1.png){fig-align=\"center\"}\n\n## Clustering {.smaller}\n\nSome use cases for clustering include:\n\n::: incremental\n-   [**Recommender systems**](https://pages.dataiku.com/recommendation-engines):\n\n    -   Grouping together users with similar viewing patterns on Netflix, in order to recommend similar content\n\n-   [**Anomaly detection**](https://pages.dataiku.com/anomaly-detection-at-scale-guidebook):\n\n    -   Fraud detection, detecting defective mechanical parts\n\n-   **Genetics**:\n\n    -   Clustering DNA patterns to analyze evolutionary biology\n\n-   **Customer segmentation**:\n\n    -   Understanding different customer segments to devise marketing strategies\n:::\n\n## Question\n\nHow well can we cluster freedom index scores?\n\n## Our data: PCA reduced Human Freedom Index {.smaller}\n\n::: {#0b863b08 .cell execution_count=21}\n``` {.python .cell-code}\ndata = pd.DataFrame(reduced_data)\ndata.rename(columns=lambda x: f'pc_{x}', inplace=True)\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pc_0</th>\n      <th>pc_1</th>\n      <th>pc_2</th>\n      <th>pc_3</th>\n      <th>pc_4</th>\n      <th>pc_5</th>\n      <th>pc_6</th>\n      <th>pc_7</th>\n      <th>pc_8</th>\n      <th>pc_9</th>\n      <th>pc_10</th>\n      <th>pc_11</th>\n      <th>pc_12</th>\n      <th>pc_13</th>\n      <th>pc_14</th>\n      <th>pc_15</th>\n      <th>pc_16</th>\n      <th>pc_17</th>\n      <th>pc_18</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.779418</td>\n      <td>-0.498626</td>\n      <td>0.469355</td>\n      <td>3.387443</td>\n      <td>0.567041</td>\n      <td>1.296449</td>\n      <td>-0.454311</td>\n      <td>0.047696</td>\n      <td>-0.150022</td>\n      <td>-1.206748</td>\n      <td>2.647105</td>\n      <td>0.583670</td>\n      <td>-0.213672</td>\n      <td>1.608173</td>\n      <td>-0.374059</td>\n      <td>-0.490913</td>\n      <td>-0.132991</td>\n      <td>-0.403151</td>\n      <td>-0.991481</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-9.374297</td>\n      <td>0.647491</td>\n      <td>-4.321327</td>\n      <td>-4.695802</td>\n      <td>-0.672385</td>\n      <td>3.209548</td>\n      <td>-0.378487</td>\n      <td>-3.215965</td>\n      <td>-1.908267</td>\n      <td>0.129144</td>\n      <td>0.595631</td>\n      <td>0.347146</td>\n      <td>-0.249731</td>\n      <td>-2.302677</td>\n      <td>-0.102451</td>\n      <td>0.297895</td>\n      <td>1.443028</td>\n      <td>-2.022844</td>\n      <td>0.943593</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-8.850868</td>\n      <td>-2.931088</td>\n      <td>2.220075</td>\n      <td>-2.100491</td>\n      <td>-0.818193</td>\n      <td>0.224287</td>\n      <td>-1.318509</td>\n      <td>0.308495</td>\n      <td>-4.324757</td>\n      <td>-1.111543</td>\n      <td>0.689022</td>\n      <td>-1.816347</td>\n      <td>1.090664</td>\n      <td>-0.404623</td>\n      <td>1.047847</td>\n      <td>0.230615</td>\n      <td>0.151937</td>\n      <td>0.683630</td>\n      <td>3.053161</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.708081</td>\n      <td>-6.713719</td>\n      <td>2.768734</td>\n      <td>-5.725613</td>\n      <td>-1.733888</td>\n      <td>-1.348572</td>\n      <td>-4.638082</td>\n      <td>-1.049769</td>\n      <td>-3.157522</td>\n      <td>0.848578</td>\n      <td>-0.132706</td>\n      <td>3.154590</td>\n      <td>-0.318503</td>\n      <td>-3.688724</td>\n      <td>-1.217743</td>\n      <td>-0.400556</td>\n      <td>3.205119</td>\n      <td>-0.621442</td>\n      <td>-1.703220</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.640691</td>\n      <td>2.223795</td>\n      <td>2.299552</td>\n      <td>0.792827</td>\n      <td>-2.449994</td>\n      <td>0.373804</td>\n      <td>2.057514</td>\n      <td>-1.241613</td>\n      <td>0.767273</td>\n      <td>0.386597</td>\n      <td>0.679367</td>\n      <td>-0.893322</td>\n      <td>-1.659838</td>\n      <td>-0.013696</td>\n      <td>0.463806</td>\n      <td>0.941999</td>\n      <td>0.872254</td>\n      <td>0.104267</td>\n      <td>-0.210407</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Clustering methods\n\n::: {style=\"text-align: center;\"}\n\n```{=html}\n<iframe width=\"1200\" height=\"400\" src=\"https://datamineaz.org/tables/model-cheatsheet.html\" frameborder=\"1\" style=\"background:white;\"></iframe>\n```\n\n:::\n\n## K-Means Clustering {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {#ea70e673 .cell execution_count=22}\n\n::: {.cell-output .cell-output-display}\n![](week5_files/figure-revealjs/cell-23-output-1.png){width=793 height=435}\n:::\n:::\n\n\n## Formula\n\n> The goal of K-Means is to minimize the variance within each cluster. The variance is measured as the sum of squared distances between each point and its corresponding cluster centroid. The objective function, which K-Means aims to minimize, can be defined as:\n\n$J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2$\n\n**Where**:\n\n::: incremental\n-   $J$ is the objective function\n\n-   $k$ is the number of clusters\n\n-   $C_i$ is the set of points belonging to a cluster $i$.\n\n-   $x$ is a point in the cluster $C_i$\n\n-   $||x - \\mu_i||^2$ is the squared Euclidean distance between a point $x$ and the centroid $\\mu_i$â€‹, which measures the dissimilarity between them.\n:::\n\n## Key points\n\n-   **Initialization**: Randomly selects $k$ initial centroids.\n\n-   **Assignment Step**: Assigns each data point to the closest centroid based on Euclidean distance.\n\n-   **Update Step**: Recalculates centroids as the mean of assigned points in each cluster.\n\n-   **Convergence**: Iterates until the centroids stabilize (minimal change from one iteration to the next).\n\n-   **Objective**: Minimizes the within-cluster sum of squares (WCSS), the sum of squared distances between points and their corresponding centroid.\n\n-   **Optimal** $k$: Determined experimentally, often using methods like the Elbow Method.\n\n-   **Sensitivity**: Results can vary based on initial centroid selection; techniques like \"k-means++\" improve initial centroid choices.\n\n-   **Efficiency**: Generally good, but worsens with increasing $k$ and data dimensionality; sensitive to outliers.\n:::\n\n## Choosing the right number of clusters {.smaller}\n\n**Four main methods:**\n\n::: incremental\n-   **Elbow Method**\n\n    -   Identifies the $k$ at which the within-cluster sum of squares (WCSS) starts to diminish more slowly.\n\n-   **Silhouette Score**\n\n    -   Measures how similar an object is to its own cluster compared to other clusters.\n\n-   **Davies-Bouldin Index**\n\n    -   Evaluates intra-cluster similarity and inter-cluster differences.\n\n-   **Calinski-Harabasz Index (Variance Ratio Criterion)**\n\n    -   Measures the ratio of the sum of between-clusters dispersion and of intra-cluster dispersion for all clusters.\n\n-   **BIC**\n\n    -   Identifies the optimal number of clusters by penalizing models for excessive parameters, striking a balance between simplicity and accuracy.\n:::\n\n## Systematic comparison: Equal clusters {.smaller}\n\n::: panel-tabset\n## Elbow\n\n![](images/equally-sized-elbow.png){fig-align=\"center\" width=\"896\"}\n\n## Davies-Boulin\n\n![](images/equally-sized-davies-bouldin.png){fig-align=\"center\" width=\"896\"}\n\n## Silhouette\n\n![](images/equally-sized-silhouette.png){fig-align=\"center\" width=\"896\"}\n\n## Calinski-Harabasz\n\n![](images/equally-sized-calinski-harabasz.png){fig-align=\"center\" width=\"896\"}\n\n## BIC\n\n![](images/equally-sized-bic.png){fig-align=\"center\" width=\"896\"}\n:::\n\n## Systematic comparison: Unequal clusters {.smaller}\n\n::: panel-tabset\n## Elbow\n\n![](images/unequally-sized-elbow.png){fig-align=\"center\" width=\"896\"}\n\n## Davies-Boulin\n\n![](images/unequally-sized-davies-bouldin.png){fig-align=\"center\" width=\"896\"}\n\n## Silhouette\n\n![](images/unequally-sized-silhouette.png){fig-align=\"center\" width=\"896\"}\n\n## Calinski-Harabasz\n\n![](images/unequally-sized-calinski-harabasz.png){fig-align=\"center\" width=\"896\"}\n\n## BIC\n\n![](images/unequally-sized-bic.png){fig-align=\"center\" width=\"896\"}\n:::\n\n## Systematic comparison - accuracy\n\n![](images/systematic-comparison.png){fig-align=\"center\" width=\"692\"}\n\n## Calinski-Harabasz Index {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {#b8ed76bb .cell execution_count=23}\n\n::: {.cell-output .cell-output-display}\n![](week5_files/figure-revealjs/cell-24-output-1.png){width=828 height=529}\n:::\n:::\n\n\n## Formula\n\n$CH = \\frac{SS_B / (k - 1)}{SS_W / (n - k)}$\n\n**where**:\n\n::: incremental\n-   $CH$ is the Calinski-Harabasz score.\n\n-   $SS_B$â€‹ is the between-cluster variance.\n\n-   $SS_W$â€‹ is the within-cluster variance.\n\n-   $k$ is the number of clusters.\n\n-   $n$ is the number of data points.\n:::\n\n## Pros + cons\n\n**Pros**:\n\n::: incremental\n-   **Clear Interpretation**: High values indicate better-defined clusters.\n\n-   **Computationally Efficient**: Less resource-intensive than many alternatives.\n\n-   **Scale-Invariant**: Effective across datasets of varying sizes.\n\n-   **No Labeled Data Required**: Useful for unsupervised learning scenarios.\n:::\n\n**Cons**:\n\n::: incremental\n-   **Cluster Structure Bias**: Prefers convex clusters of similar sizes.\n\n-   **Sample Size Sensitivity**: Can favor more clusters in larger datasets.\n\n-   **Not Ideal for Overlapping Clusters**: Assumes distinct, non-overlapping clusters.\n:::\n:::\n\n## BIC {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {#66482f5a .cell execution_count=24}\n\n::: {.cell-output .cell-output-display}\n![](week5_files/figure-revealjs/cell-25-output-1.png){width=828 height=529}\n:::\n:::\n\n\n## Formula\n\n$\\text{BIC} = -2 \\ln(\\hat{L}) + k \\ln(n)$\n\n**where**:\n\n::: incremental\n-   $\\hat{L}$ is the maximized value of the likelihood function of the model,\n\n-   $k$ is the number of parameters in the model,\n\n-   $n$ is the number of observations.\n:::\n\n## Pros + cons\n\n**Pros**:\n\n::: incremental\n-   **Penalizes Complexity**: Helps avoid overfitting by penalizing models with more parameters.\n\n-   **Objective Selection**: Facilitates choosing the model with the best balance between fit and simplicity.\n\n-   **Applicability**: Useful across various model types, including clustering and regression.\n:::\n\n**Cons**:\n\n::: incremental\n-   **Computationally Intensive**: Requires fitting multiple models to calculate, which can be resource-heavy.\n\n-   **Sensitivity to Model Assumptions**: Performance depends on the underlying assumptions of the model being correct.\n\n-   **Not Always Intuitive**: Determining the absolute best model may still require domain knowledge and additional diagnostics.\n:::\n:::\n\n## K-Means Clustering: applied {.smaller}\n\n::: panel-tabset\n## Calinski-Harabasz Index {.smaller}\n\n::: {#9660bb3f .cell execution_count=25}\n``` {.python .cell-code code-fold=\"true\"}\n# Finding the optimal number of clusters using Calinski-Harabasz Index\ncalinski_harabasz_scores = []\ncluster_range = range(2, 11)  # Define the range for number of clusters\n\nfor n_clusters in cluster_range:\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n    kmeans.fit(data)\n    labels = kmeans.labels_\n    score = calinski_harabasz_score(data, labels)\n    calinski_harabasz_scores.append(score)\n\n# Plotting the Calinski-Harabasz scores\nplt.plot(cluster_range, calinski_harabasz_scores, marker='o')\nplt.title('Calinski-Harabasz Index for Different Numbers of Clusters')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Calinski-Harabasz Index')\nplt.grid(True)\nplt.show()\n\n# Finding the number of clusters that maximizes the Calinski-Harabasz Index\noptimal_n_clusters = cluster_range[calinski_harabasz_scores.index(max(calinski_harabasz_scores))]\nprint(f\"The optimal number of clusters is: {optimal_n_clusters}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](week5_files/figure-revealjs/cell-26-output-1.png){width=820 height=455}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nThe optimal number of clusters is: 2\n```\n:::\n:::\n\n\n## Model summary\n\n::: {#8c95ebfd .cell execution_count=26}\n``` {.python .cell-code code-fold=\"true\"}\n# K-Means Clustering with the optimal number of clusters\nkmeans = KMeans(n_clusters=optimal_n_clusters, random_state=0)\nkmeans.fit(data)\nclusters = kmeans.predict(data)\n\n# Adding cluster labels to the DataFrame\ndata['Cluster'] = clusters\n\n# Model Summary\nprint(\"Cluster Centers:\\n\", kmeans.cluster_centers_)\n\n# Evaluate clustering performance using the Calinski-Harabasz Index\ncalinski_harabasz_score_final = calinski_harabasz_score(data.drop(columns='Cluster'), clusters)\nprint(f\"For n_clusters = {optimal_n_clusters}, the Calinski-Harabasz Index is : {calinski_harabasz_score_final:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster Centers:\n [[ 6.81306346  0.19993757 -0.28806769 -0.29502272 -0.46716703  0.05454598\n  -0.12994433  0.03657521 -0.02176455  0.03151719  0.24393655 -0.01449253\n  -0.03272898  0.04760448 -0.01648803 -0.03251455  0.01440017  0.05300893\n  -0.01630804]\n [-3.57757098 -0.10498814  0.15126567  0.15491779  0.24531156 -0.02864235\n   0.06823436 -0.01920581  0.01142867 -0.01654982 -0.12809221  0.0076101\n   0.01718614 -0.02499733  0.00865794  0.01707354 -0.00756159 -0.02783523\n   0.00856343]]\nFor n_clusters = 2, the Calinski-Harabasz Index is : 536.957\n```\n:::\n:::\n\n\n## Visualize results\n\n::: {#1ab67dee .cell execution_count=27}\n``` {.python .cell-code code-fold=\"true\"}\npca = PCA(n_components = 2)\nreduced_data_PCA = pca.fit_transform(data)\nsns.scatterplot(x = reduced_data_PCA[:, 0], y = reduced_data_PCA[:, 1], hue = clusters, alpha = 0.75, palette = \"colorblind\")\nplt.title('Human Freedom Index Clustered (PCA-reduced Features)')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend(title = 'Cluster')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](week5_files/figure-revealjs/cell-28-output-1.png){width=821 height=455}\n:::\n:::\n\n\n:::\n\n## Caveat\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {#25ffeb51 .cell execution_count=28}\n\n::: {.cell-output .cell-output-display}\n![](week5_files/figure-revealjs/cell-29-output-1.png){width=470 height=307}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {#b42769d6 .cell execution_count=29}\n\n::: {.cell-output .cell-output-display}\n![](week5_files/figure-revealjs/cell-30-output-1.png){width=692 height=307}\n:::\n:::\n\n\n:::\n:::\n\n## Conclusions {.smaller}\n\n::: incremental\n-   **Clear Separation:**\n\n    -   Two distinct clusters (Cluster 0 and Cluster 1) are evident, indicating effective separation by PCA.\n\n-   **Cluster Characteristics:**\n\n    -   Cluster 1 (orange) is compact and concentrated around the origin.\n\n    -   Cluster 0 (blue) is more spread out across the PCA axes.\n\n-   **Slight Overlap:**\n\n    -   There is a transition zone between the clusters, suggesting some borderline cases.\n\n-   **PCA Components:**\n\n    -   The axes represent the first two principal components, highlighting significant differences in the clusters.\n\n-   **Implications:**\n\n    -   The clusters likely reflect differences in Human Freedom Index scores, with further analysis needed to understand specific feature contributions.\n:::\n\n",
    "supporting": [
      "week5_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}